{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc268d6-4fc2-43f8-8589-7964adefdc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-08 10:46:09--  https://github.com/open-mmlab/mmdetection/blob/main/demo/demo.mp4?raw=true\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/open-mmlab/mmdetection/raw/refs/heads/main/demo/demo.mp4 [following]\n",
      "--2025-07-08 10:46:09--  https://github.com/open-mmlab/mmdetection/raw/refs/heads/main/demo/demo.mp4\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/open-mmlab/mmdetection/refs/heads/main/demo/demo.mp4 [following]\n",
      "--2025-07-08 10:46:09--  https://raw.githubusercontent.com/open-mmlab/mmdetection/refs/heads/main/demo/demo.mp4\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 297688 (291K) [application/octet-stream]\n",
      "Saving to: ‘demo.mp4’\n",
      "\n",
      "demo.mp4            100%[===================>] 290.71K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-07-08 10:46:09 (3.81 MB/s) - ‘demo.mp4’ saved [297688/297688]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/open-mmlab/mmdetection/blob/main/demo/demo.mp4?raw=true -O demo.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c838153-7b2c-4f45-a718-06a03cdba91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir temp\n",
    "!mv demo.mp4 temp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89cba54-64d1-4d38-bad3-f8d7af165160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "video_path = 'temp/demo.mp4'\n",
    "output_dir = 'video_frames'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "i = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imwrite(f\"{output_dir}/frame_{i:03d}.jpg\", frame)\n",
    "    i += 1\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e30ff05-5f72-422b-859f-4fd9bc77e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_000.jpg  frame_014.jpg  frame_028.jpg  frame_042.jpg  frame_056.jpg\n",
      "frame_001.jpg  frame_015.jpg  frame_029.jpg  frame_043.jpg  frame_057.jpg\n",
      "frame_002.jpg  frame_016.jpg  frame_030.jpg  frame_044.jpg  frame_058.jpg\n",
      "frame_003.jpg  frame_017.jpg  frame_031.jpg  frame_045.jpg  frame_059.jpg\n",
      "frame_004.jpg  frame_018.jpg  frame_032.jpg  frame_046.jpg  frame_060.jpg\n",
      "frame_005.jpg  frame_019.jpg  frame_033.jpg  frame_047.jpg  frame_061.jpg\n",
      "frame_006.jpg  frame_020.jpg  frame_034.jpg  frame_048.jpg  frame_062.jpg\n",
      "frame_007.jpg  frame_021.jpg  frame_035.jpg  frame_049.jpg  frame_063.jpg\n",
      "frame_008.jpg  frame_022.jpg  frame_036.jpg  frame_050.jpg  frame_064.jpg\n",
      "frame_009.jpg  frame_023.jpg  frame_037.jpg  frame_051.jpg  frame_065.jpg\n",
      "frame_010.jpg  frame_024.jpg  frame_038.jpg  frame_052.jpg  frame_066.jpg\n",
      "frame_011.jpg  frame_025.jpg  frame_039.jpg  frame_053.jpg\n",
      "frame_012.jpg  frame_026.jpg  frame_040.jpg  frame_054.jpg\n",
      "frame_013.jpg  frame_027.jpg  frame_041.jpg  frame_055.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100ce459-2f80-4ab3-8b72-d92a64aa1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-08 10:46:14--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/fast_rcnn/fast-rcnn_r50_fpn_1x_coco.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1353 (1.3K) [text/plain]\n",
      "Saving to: ‘fast-rcnn_r50_fpn_1x_coco.py’\n",
      "\n",
      "fast-rcnn_r50_fpn_1 100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:15 (38.4 MB/s) - ‘fast-rcnn_r50_fpn_1x_coco.py’ saved [1353/1353]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/fast_rcnn/fast-rcnn_r50_fpn_1x_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e525d93-ffa5-4f69-a525-48b81c36592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-08 10:46:20--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/models/fast-rcnn_r50_fpn.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2256 (2.2K) [text/plain]\n",
      "Saving to: ‘configs/_base_/models/fast-rcnn_r50_fpn.py’\n",
      "\n",
      "fast-rcnn_r50_fpn.p 100%[===================>]   2.20K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:21 (35.0 MB/s) - ‘configs/_base_/models/fast-rcnn_r50_fpn.py’ saved [2256/2256]\n",
      "\n",
      "--2025-07-08 10:46:21--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/datasets/coco_detection.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3187 (3.1K) [text/plain]\n",
      "Saving to: ‘configs/_base_/datasets/coco_detection.py’\n",
      "\n",
      "coco_detection.py   100%[===================>]   3.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:21 (61.3 MB/s) - ‘configs/_base_/datasets/coco_detection.py’ saved [3187/3187]\n",
      "\n",
      "--2025-07-08 10:46:21--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/schedules/schedule_1x.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 814 [text/plain]\n",
      "Saving to: ‘configs/_base_/schedules/schedule_1x.py’\n",
      "\n",
      "schedule_1x.py      100%[===================>]     814  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:21 (64.4 MB/s) - ‘configs/_base_/schedules/schedule_1x.py’ saved [814/814]\n",
      "\n",
      "--2025-07-08 10:46:21--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/default_runtime.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 759 [text/plain]\n",
      "Saving to: ‘configs/_base_/default_runtime.py’\n",
      "\n",
      "default_runtime.py  100%[===================>]     759  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:22 (62.0 MB/s) - ‘configs/_base_/default_runtime.py’ saved [759/759]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create folders\n",
    "!mkdir -p configs/_base_/models\n",
    "!mkdir -p configs/_base_/datasets\n",
    "!mkdir -p configs/_base_/schedules\n",
    "\n",
    "# Download the required base files\n",
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/models/fast-rcnn_r50_fpn.py -P configs/_base_/models/\n",
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/datasets/coco_detection.py -P configs/_base_/datasets/\n",
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/schedules/schedule_1x.py -P configs/_base_/schedules/\n",
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/default_runtime.py -P configs/_base_/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3009ea-b132-4f1c-9353-e7f2286b7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-08 10:46:28--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/models/faster-rcnn_r50_fpn.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3828 (3.7K) [text/plain]\n",
      "Saving to: ‘configs/_base_/models/faster-rcnn_r50_fpn.py’\n",
      "\n",
      "faster-rcnn_r50_fpn 100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:28 (83.7 MB/s) - ‘configs/_base_/models/faster-rcnn_r50_fpn.py’ saved [3828/3828]\n",
      "\n",
      "--2025-07-08 10:46:28--  https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 177 [text/plain]\n",
      "Saving to: ‘configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py’\n",
      "\n",
      "faster-rcnn_r50_fpn 100%[===================>]     177  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-08 10:46:28 (6.27 MB/s) - ‘configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py’ saved [177/177]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/_base_/models/faster-rcnn_r50_fpn.py -P configs/_base_/models/\n",
    "!wget https://raw.githubusercontent.com/open-mmlab/mmdetection/main/configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py -P configs/faster_rcnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f612b75d-e712-43e9-88f7-6986091a030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster-rcnn_r50_fpn_1x_coco.py\n"
     ]
    }
   ],
   "source": [
    "#!ls configs/_base_/models\n",
    "!ls configs/faster_rcnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3273b5ea-90cd-4804-94db-91bc8570fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-08 10:46:38--  https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 163.181.60.223, 163.181.60.220, 163.181.60.219, ...\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|163.181.60.223|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 167287506 (160M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth’\n",
      "\n",
      "faster_rcnn_r50_fpn 100%[===================>] 159.54M  14.3MB/s    in 11s     \n",
      "\n",
      "2025-07-08 10:46:50 (14.1 MB/s) - ‘checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth’ saved [167287506/167287506]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth -P checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d89f055-7a95-479c-8c76-f03ba4229052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/08 10:47:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - torch2onnx: \n",
      "\tmodel_cfg: configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \n",
      "\tdeploy_cfg: /mmdeploy/configs/mmdet/detection/detection_onnxruntime_dynamic.py\n",
      "07/08 10:47:22 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"Codebases\" registry tree. As a workaround, the current \"Codebases\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n",
      "07/08 10:47:22 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"mmdet_tasks\" registry tree. As a workaround, the current \"mmdet_tasks\" registry in \"mmdeploy\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n",
      "Loads checkpoint by local backend from path: checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n",
      "07/08 10:47:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - DeprecationWarning: get_onnx_config will be deprecated in the future. \n",
      "07/08 10:47:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Export PyTorch model to ONNX: output_models/end2end.onnx.\n",
      "/mmdeploy/mmdeploy/core/optimizers/function_marker.py:160: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  ys_shape = tuple(int(s) for s in ys.shape)\n",
      "/usr/local/lib/python3.10/dist-packages/mmdet/models/dense_heads/anchor_head.py:115: UserWarning: DeprecationWarning: anchor_generator is deprecated, please use \"prior_generator\" instead\n",
      "  warnings.warn('DeprecationWarning: anchor_generator is deprecated, '\n",
      "/usr/local/lib/python3.10/dist-packages/mmdet/models/task_modules/prior_generators/anchor_generator.py:356: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n",
      "  warnings.warn('``grid_anchors`` would be deprecated soon. '\n",
      "/usr/local/lib/python3.10/dist-packages/mmdet/models/task_modules/prior_generators/anchor_generator.py:392: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n",
      "  warnings.warn(\n",
      "/mmdeploy/mmdeploy/codebase/mmdet/models/dense_heads/rpn_head.py:89: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n",
      "/mmdeploy/mmdeploy/pytorch/functions/topk.py:28: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  k = torch.tensor(k, device=input.device, dtype=torch.long)\n",
      "/mmdeploy/mmdeploy/codebase/mmdet/models/task_modules/coders/delta_xywh_bbox_coder.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert pred_bboxes.size(0) == bboxes.size(0)\n",
      "/mmdeploy/mmdeploy/codebase/mmdet/models/task_modules/coders/delta_xywh_bbox_coder.py:40: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert pred_bboxes.size(1) == bboxes.size(1)\n",
      "/mmdeploy/mmdeploy/codebase/mmdet/deploy/utils.py:48: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  assert len(max_shape) == 2, '`max_shape` should be [h, w]'\n",
      "/mmdeploy/mmdeploy/mmcv/ops/nms.py:285: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  iou_threshold = torch.tensor([iou_threshold], dtype=torch.float32)\n",
      "/mmdeploy/mmdeploy/mmcv/ops/nms.py:286: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  score_threshold = torch.tensor([score_threshold], dtype=torch.float32)\n",
      "/mmdeploy/mmdeploy/mmcv/ops/nms.py:44: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  score_threshold = float(score_threshold)\n",
      "/mmdeploy/mmdeploy/mmcv/ops/nms.py:45: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  iou_threshold = float(iou_threshold)\n",
      "/mmcv/mmcv/ops/nms.py:123: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert boxes.size(1) == 4\n",
      "/mmcv/mmcv/ops/nms.py:124: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert boxes.size(0) == scores.size(0)\n",
      "/mmdeploy/mmdeploy/codebase/mmdet/models/roi_heads/standard_roi_head.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  rois_dims = int(rois.shape[-1])\n",
      "/mmcv/mmcv/ops/roi_align.py:78: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert rois.size(1) == 5, 'RoI must be (idx, x1, y1, x2, y2)!'\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5589: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "/mmdeploy/mmdeploy/mmcv/ops/roi_align.py:61: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Long' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  batch_indices = _cast_Long(\n",
      "07/08 10:47:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Execute onnx optimize passes.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "07/08 10:47:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - torch2onnx finished. Results saved to output_models\n"
     ]
    }
   ],
   "source": [
    "import mmdeploy\n",
    "!python /mmdeploy/tools/torch2onnx.py /mmdeploy/configs/mmdet/detection/detection_onnxruntime_dynamic.py configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth video_frames/frame_000.jpg --device cuda --work-dir output_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc557a3-c1a1-4a13-ab05-5c4504a6d57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end2end.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls output_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceeca1f5-e2d3-41d8-8c1c-bc1e82f62e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv output_models/end2end.onnx output_models/model.onnx\n",
    "!chmod 744 create_model_repo.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f42ba77-43a9-462a-8d65-1a0a29052cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 720)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"video_frames/frame_000.jpg\")\n",
    "print(img.size)  # (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa09421e-d61a-48d5-8a4e-b139d32bb9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "input [0, 3, 0, 0]\n",
      "Outputs:\n",
      "dets [0, 0, 0]\n",
      "labels [0, 0]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"output_models/model.onnx\")\n",
    "print(\"Inputs:\")\n",
    "for input in model.graph.input:\n",
    "    print(input.name, [dim.dim_value for dim in input.type.tensor_type.shape.dim])\n",
    "\n",
    "print(\"Outputs:\")\n",
    "for output in model.graph.output:\n",
    "    print(output.name, [dim.dim_value for dim in output.type.tensor_type.shape.dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e96e4b23-f7dc-4872-b1c7-8dd60efb7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model repository created successfully in: model_repository\n"
     ]
    }
   ],
   "source": [
    "!./create_model_repo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19df4e3-5cf9-4ca2-b2c8-5683ce18de18",
   "metadata": {},
   "source": [
    "### Now go ahead and start the triton server before running the next cell\n",
    "```\n",
    "tritonserver --model-repository=/workspace/model_repository/ --log-verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f3b446-3498-4f43-a5d6-034c8a1a88c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 61\n",
      "Batch size: 4\n",
      "Total batches: 16\n",
      "Latency per image: 167.49 ms\n",
      "Throughput: 5.97 FPS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mmcv\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# Triton client\n",
    "client = httpclient.InferenceServerClient(\"localhost:8000\")\n",
    "\n",
    "# Model name\n",
    "model_name = \"faster_rcnn\"\n",
    "\n",
    "# Input/output directories\n",
    "input_dir = './video_frames'\n",
    "output_dir = './output_frames'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Batching parameters\n",
    "batch_size = 4\n",
    "warmup_cnt = 6\n",
    "\n",
    "# Gather list of images\n",
    "img_files = [f for f in sorted(os.listdir(input_dir)) if f.lower().endswith('.jpg')]\n",
    "img_files = img_files[warmup_cnt:]  # Skip warmup images\n",
    "\n",
    "n_runs = len(img_files)\n",
    "n_batches = (n_runs + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Total images: {n_runs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Total batches: {n_batches}\")\n",
    "\n",
    "total_time = 0.0\n",
    "\n",
    "# Process batches\n",
    "for batch_idx in range(n_batches):\n",
    "    batch_imgs = img_files[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "    \n",
    "    batch_data = []\n",
    "    for img_name in batch_imgs:\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        image = mmcv.imread(img_path).astype(np.float32)\n",
    "\n",
    "        # Preprocess: transpose and ensure shape (C,H,W)\n",
    "        image = np.transpose(image, (2,0,1))\n",
    "        batch_data.append(image)\n",
    "\n",
    "    # Convert to single batch array\n",
    "    input_data = np.stack(batch_data, axis=0)  # shape: (B,C,H,W)\n",
    "\n",
    "    # Create input object\n",
    "    inputs = [httpclient.InferInput(\"input\", input_data.shape, \"FP32\")]\n",
    "    inputs[0].set_data_from_numpy(input_data)\n",
    "\n",
    "    # Requested outputs\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"dets\"),\n",
    "        httpclient.InferRequestedOutput(\"labels\")\n",
    "    ]\n",
    "\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "    response = client.infer(model_name, inputs=inputs, outputs=outputs)\n",
    "    end = time.time()\n",
    "    total_time += (end - start)\n",
    "\n",
    "    # Extract batched results\n",
    "    dets_batch = response.as_numpy(\"dets\")\n",
    "    labels_batch = response.as_numpy(\"labels\")\n",
    "\n",
    "    # You can iterate over the batch to process individual outputs\n",
    "    for i, img_name in enumerate(batch_imgs):\n",
    "        dets = dets_batch[i]\n",
    "        labels = labels_batch[i]\n",
    "        # Example: print(f\"{img_name} - dets shape: {dets.shape}\")\n",
    "\n",
    "# Stats\n",
    "latency_ms = total_time / n_runs * 1000\n",
    "throughput = n_runs / total_time\n",
    "\n",
    "print(f\"Latency per image: {latency_ms:.2f} ms\")\n",
    "print(f\"Throughput: {throughput:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a32b9ad8-c5d4-4d66-a167-cdd3f12c578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 61\n",
      "Batch size: 2\n",
      "Total batches: 31\n",
      "Latency per image: 124.52 ms\n",
      "Throughput: 8.03 FPS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model name\n",
    "model_name = \"faster_rcnn_config_3\"\n",
    "\n",
    "# Input/output directories\n",
    "input_dir = './video_frames'\n",
    "output_dir = './output_frames'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Batching parameters\n",
    "#batch_size = 4\n",
    "batch_size = 2\n",
    "warmup_cnt = 6\n",
    "\n",
    "# Gather list of images\n",
    "img_files = [f for f in sorted(os.listdir(input_dir)) if f.lower().endswith('.jpg')]\n",
    "img_files = img_files[warmup_cnt:]  # Skip warmup images\n",
    "\n",
    "n_runs = len(img_files)\n",
    "n_batches = (n_runs + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"Total images: {n_runs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Total batches: {n_batches}\")\n",
    "\n",
    "total_time = 0.0\n",
    "\n",
    "# Process batches\n",
    "for batch_idx in range(n_batches):\n",
    "    batch_imgs = img_files[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "    \n",
    "    batch_data = []\n",
    "    for img_name in batch_imgs:\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        image = mmcv.imread(img_path).astype(np.float32)\n",
    "\n",
    "        # Preprocess: transpose and ensure shape (C,H,W)\n",
    "        image = np.transpose(image, (2,0,1))\n",
    "        batch_data.append(image)\n",
    "\n",
    "    # Convert to single batch array\n",
    "    input_data = np.stack(batch_data, axis=0)  # shape: (B,C,H,W)\n",
    "\n",
    "    # Create input object\n",
    "    inputs = [httpclient.InferInput(\"input\", input_data.shape, \"FP32\")]\n",
    "    inputs[0].set_data_from_numpy(input_data)\n",
    "\n",
    "    # Requested outputs\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"dets\"),\n",
    "        httpclient.InferRequestedOutput(\"labels\")\n",
    "    ]\n",
    "\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "    response = client.infer(model_name, inputs=inputs, outputs=outputs)\n",
    "    end = time.time()\n",
    "    total_time += (end - start)\n",
    "\n",
    "    # Extract batched results\n",
    "    dets_batch = response.as_numpy(\"dets\")\n",
    "    labels_batch = response.as_numpy(\"labels\")\n",
    "\n",
    "    # You can iterate over the batch to process individual outputs\n",
    "    for i, img_name in enumerate(batch_imgs):\n",
    "        dets = dets_batch[i]\n",
    "        labels = labels_batch[i]\n",
    "        # Example: print(f\"{img_name} - dets shape: {dets.shape}\")\n",
    "\n",
    "# Stats\n",
    "latency_ms = total_time / n_runs * 1000\n",
    "throughput = n_runs / total_time\n",
    "\n",
    "print(f\"Latency per image: {latency_ms:.2f} ms\")\n",
    "print(f\"Throughput: {throughput:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8480bc74-77e1-4ad6-8dfb-61e0f278e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 0 latency: 2757.48 ms\n",
      "Request 32 latency: 2659.05 ms\n",
      "Request 64 latency: 2684.58 ms\n",
      "Request 96 latency: 2651.76 ms\n",
      "Request 128 latency: 2667.71 ms\n",
      "Request 160 latency: 2687.44 ms\n",
      "Request 192 latency: 2680.22 ms\n",
      "Request 224 latency: 2772.81 ms\n",
      "\n",
      "Final Results:\n",
      "Total requests: 256\n",
      "Batch size: 2\n",
      "Max concurrency: 16\n",
      "Avg latency per request: 2762.54 ms\n",
      "Throughput (FPS): 10.92\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import time\n",
    "import tritonclient.grpc.aio as grpcclient\n",
    "from tritonclient.grpc import InferInput, InferRequestedOutput\n",
    "\n",
    "nest_asyncio.apply()  # fix event loop issues in Jupyter\n",
    "\n",
    "model_name = \"faster_rcnn_config_2\"\n",
    "grpc_url = \"localhost:8001\"\n",
    "batch_size = 2\n",
    "input_shape = (3, 800, 1333)  # Change per your model\n",
    "dtype = \"FP32\"\n",
    "\n",
    "total_requests = 256\n",
    "max_concurrency = 16  # Start small, increase as stable\n",
    "\n",
    "# Semaphore to limit concurrency\n",
    "semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "def generate_synthetic_batch():\n",
    "    input_data = np.random.rand(batch_size, *input_shape).astype(np.float32)\n",
    "    inputs = [InferInput(\"input\", input_data.shape, dtype)]\n",
    "    inputs[0].set_data_from_numpy(input_data)\n",
    "    outputs = [InferRequestedOutput(\"dets\"), InferRequestedOutput(\"labels\")]\n",
    "    return inputs, outputs\n",
    "\n",
    "async def infer_once(client, request_id):\n",
    "    async with semaphore:\n",
    "        inputs, outputs = generate_synthetic_batch()\n",
    "        start = time.time()\n",
    "        response = await client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "        end = time.time()\n",
    "        latency = end - start\n",
    "        # Optionally print progress every 32 requests\n",
    "        if request_id % 32 == 0:\n",
    "            print(f\"Request {request_id} latency: {latency*1000:.2f} ms\")\n",
    "        return latency\n",
    "\n",
    "async def main():\n",
    "    async with grpcclient.InferenceServerClient(url=grpc_url) as client:\n",
    "        tasks = [infer_once(client, i) for i in range(total_requests)]\n",
    "        start_all = time.time()\n",
    "        latencies = await asyncio.gather(*tasks)\n",
    "        total_time = time.time() - start_all\n",
    "\n",
    "        total_images = total_requests * batch_size\n",
    "        avg_latency_ms = np.mean(latencies) * 1000\n",
    "        throughput = total_images / total_time\n",
    "\n",
    "        print(\"\\nFinal Results:\")\n",
    "        print(f\"Total requests: {total_requests}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Max concurrency: {max_concurrency}\")\n",
    "        print(f\"Avg latency per request: {avg_latency_ms:.2f} ms\")\n",
    "        print(f\"Throughput (FPS): {throughput:.2f}\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d0c00-3a15-4894-8dde-d4b6506038c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
